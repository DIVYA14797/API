{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWU/+frmXGCdGsNsFWWxaZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DIVYA14797/API/blob/main/Boosting_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxU5Mo3kZRr8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Gradient Boosting Regression (GBR) ?"
      ],
      "metadata": {
        "id": "iBfc-CSxZSVy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Boosting Regression (GBR) is a machine learning technique used for regression tasks, meaning it predicts continuous numeric values. It's an ensemble learning method where a sequence of weak learners (usually decision trees) are combined to build a strong learner."
      ],
      "metadata": {
        "id": "a-Bk8_rXZUzv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cFMoT9qLaucf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Implement a simple GB algorithm from scratch using python and Numpy . use a simple regression problem as an example and train the model on a small dataset . Evaluate the model's performance using metrics such as mean squared error and R-squared ?\n",
        "\n"
      ],
      "metadata": {
        "id": "zI5udSGSavRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "class GradientBoostingRegressor:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.models = []\n",
        "        self.residuals = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        y_pred = np.full_like(y, np.mean(y))  # Initial prediction: mean of y\n",
        "        for _ in range(self.n_estimators):\n",
        "            residuals = y - y_pred\n",
        "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
        "            tree.fit(X, residuals)\n",
        "            self.models.append(tree)\n",
        "            self.residuals.append(tree.predict(X))\n",
        "            y_pred += self.learning_rate * self.residuals[-1]\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = np.full(X.shape[0], np.mean(self.residuals))\n",
        "        for i in range(len(self.models)):\n",
        "            y_pred += self.learning_rate * self.models[i].predict(X)\n",
        "        return y_pred\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the Gradient Boosting Regression model\n",
        "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
        "gbr.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = gbr.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuRJvICHa5Qu",
        "outputId": "25729c25-e629-431a-c7f1-83511af2dc53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 31.735482349161565\n",
            "R-squared: 0.9772379183627112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u5FfmGZ8bGAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Experiment with different hyperparameters such as learning rate , number of trees and tree depth to optimize the performance of the model . Use grid search or random search to find the best hyperparameters.  \n",
        "\n"
      ],
      "metadata": {
        "id": "fUM-9JWIbGnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n",
        "\n",
        "# Define parameter grid for RandomizedSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [2, 3, 4]\n",
        "}\n",
        "\n",
        "# Initialize GradientBoostingRegressor\n",
        "gbr = GradientBoostingRegressor()\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(gbr, param_distributions=param_grid, n_iter=10, cv=5, random_state=42)\n",
        "\n",
        "# Perform RandomizedSearchCV\n",
        "random_search.fit(X, y)\n",
        "\n",
        "# Get best hyperparameters\n",
        "best_params = random_search.best_params_\n",
        "\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# Evaluate the model with best hyperparameters\n",
        "best_gbr = GradientBoostingRegressor(**best_params)\n",
        "best_gbr.fit(X, y)\n",
        "y_pred = best_gbr.predict(X)\n",
        "\n",
        "mse = mean_squared_error(y, y_pred)\n",
        "r2 = r2_score(y, y_pred)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IY5aI8hwcOQT",
        "outputId": "929ba3f1-7f51-4271-ae58-948afad8203b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'n_estimators': 50, 'max_depth': 4, 'learning_rate': 0.2}\n",
            "Mean Squared Error: 0.0027613446807511445\n",
            "R-squared: 0.9999980615186944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zry6mu-JcTT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the weak learner in  Gradient  Boosting ?"
      ],
      "metadata": {
        "id": "E7ukXf6xcT1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Gradient Boosting, the weak learner typically refers to a base model that performs slightly better than random guessing but is not necessarily strong enough to solve the entire problem on its own. The most commonly used weak learner in Gradient Boosting algorithms is a decision tree, particularly a shallow decision tree."
      ],
      "metadata": {
        "id": "sbYPdWyGcad5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LscF05EmciVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is the intuition behind the GB algorithm ?\n",
        "\n"
      ],
      "metadata": {
        "id": "nNCe8VRecixz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The intuition behind Gradient Boosting (GB) algorithm lies in the idea of improving model predictions iteratively by focusing on the residuals, or errors, made by the previous models. Here's a breakdown of the intuition behind GB:\n",
        "\n",
        "1. Sequential Improvement: GB builds an ensemble model iteratively, with each new model focusing on reducing the errors made by the previous ones. It sequentially adds new weak learners to correct the mistakes of the existing ensemble.\n",
        "\n",
        "2. Gradient Descent: GB is based on the principle of gradient descent optimization. Instead of optimizing the parameters of a single complex model directly, GB optimizes the model by adding new models in the direction that minimizes the loss function, typically the gradient of the loss function.\n",
        "\n",
        "3. Residual Fitting: At each iteration, GB fits a new weak learner (usually a decision tree) to the residuals (the differences between the predicted values and the actual values). By focusing on the residuals, the new model learns to capture the patterns that were missed by the previous models.\n",
        "\n",
        "4. Emphasis on Misclassifications: GB places more emphasis on samples that were incorrectly predicted by the ensemble so far. This allows the algorithm to gradually correct its mistakes and improve its overall performance.\n",
        "\n",
        "5. Combining Weak Models: Although individual weak learners might not perform well on their own, when combined through boosting, they can lead to a strong predictive model. Each weak learner contributes a small piece of the puzzle, and together they form a more accurate and robust ensemble.\n",
        "\n",
        "6. Regularization: GB inherently applies regularization by adding new models in a way that reduces overfitting. By using shallow trees and controlling the learning rate, GB prevents the ensemble from memorizing the training data and generalizes better to unseen data.\n",
        "\n",
        "In summary, the intuition behind Gradient Boosting is to iteratively improve model predictions by fitting new models to the errors of the previous ones, gradually reducing the overall error and building a strong ensemble model."
      ],
      "metadata": {
        "id": "WIH9EvLkcpm_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9j4143q3c5hX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. How does GB algorithm build an ensemble of weak learners ?"
      ],
      "metadata": {
        "id": "eakCk7Wfc6kk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Boosting (GB) algorithm builds an ensemble of weak learners through a sequential process. Here's a step-by-step explanation of how it constructs the ensemble:\n",
        "\n",
        "1. Initialization: The algorithm starts with an initial prediction, usually the mean of the target variable for regression tasks or the log-odds for classification tasks.\n",
        "\n",
        "2. Iterative Process:\n",
        "\n",
        "* Compute Residuals: The algorithm computes the residuals by subtracting the current prediction from the actual target values. These residuals represent the errors made by the current ensemble model.\n",
        "* Fit Weak Learner: A weak learner, often a decision tree, is fitted to the residuals. This weak learner is trained to capture the patterns in the residuals, effectively reducing the error of the ensemble.\n",
        "* Update Ensemble Predictions: The predictions of the weak learner are scaled by a factor (learning rate) and added to the current ensemble predictions. This adjustment is made in the direction that minimizes the loss function.\n",
        "* Repeat: Steps 2-3 are repeated for a predefined number of iterations or until a certain level of performance is reached.\n",
        "\n",
        "3.  Final Ensemble Prediction: The final ensemble prediction is the sum of the initial prediction and the predictions of all weak learners weighted by their learning rates.\n",
        "\n",
        "4. Regularization: GB applies regularization techniques such as controlling the learning rate and using shallow trees to prevent overfitting and improve generalization performance.\n",
        "\n",
        "5. Ensemble Combination: The weak learners are combined through addition, each contributing its own prediction to the final ensemble. While individual weak learners may not perform well on their own, their combination through boosting results in a strong ensemble model.\n",
        "\n",
        "In summary, Gradient Boosting algorithm builds an ensemble of weak learners by sequentially fitting new models to the errors of the current ensemble, gradually reducing the overall error and constructing a powerful predictive model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2zRGkP3GdDMC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LI3-QaN0dlzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. what are the steps involved in constructing the mathematical intuition of GB algorithm ?\n",
        "\n"
      ],
      "metadata": {
        "id": "YptcKKl6dmaV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Constructing the mathematical intuition behind the Gradient Boosting (GB) algorithm involves understanding the underlying principles of gradient descent optimization and ensemble learning. Here are the steps involved in constructing the mathematical intuition of the GB algorithm:\n",
        "\n",
        "1. Loss Function Optimization:\n",
        "\n",
        "* Start by defining a loss function that measures the discrepancy between the predicted values and the actual target values. This loss function should be differentiable with respect to the predictions.\n",
        "* The goal of GB is to minimize this loss function by iteratively adding new weak learners to the ensemble.\n",
        "* Common loss functions for regression tasks include mean squared error (MSE) and absolute error, while for classification tasks, cross-entropy loss is often used.\n",
        "\n",
        "2. Gradient Descent Optimization:\n",
        "\n",
        "* Gradient Boosting is based on the principle of gradient descent optimization.\n",
        "* At each iteration, GB fits a new weak learner to the negative gradient of the loss function with respect to the predictions of the current ensemble.\n",
        "* This means that the weak learner is trained to capture the direction in which the loss function decreases the fastest, effectively reducing the error of the ensemble.\n",
        "\n",
        "3. Residual Fitting:\n",
        "\n",
        "* GB fits the weak learner to the residuals, which are the differences between the actual target values and the predictions of the current ensemble.\n",
        "* By focusing on the residuals, the new weak learner learns to capture the patterns that were missed by the previous models in the ensemble.\n",
        "\n",
        "4. Learning Rate:\n",
        "\n",
        "* GB introduces a learning rate parameter to control the contribution of each weak learner to the ensemble.\n",
        "* The predictions of the weak learner are scaled by the learning rate before being added to the current ensemble predictions.\n",
        "* A smaller learning rate slows down the learning process but can lead to better generalization.\n",
        "\n",
        "5. Ensemble Combination:\n",
        "\n",
        "* The weak learners are combined through addition, with each weak learner contributing its own prediction to the final ensemble.\n",
        "* While individual weak learners may not perform well on their own, their combination through boosting results in a strong ensemble model.\n",
        "\n",
        "6. Regularization:\n",
        "\n",
        "* GB applies regularization techniques to prevent overfitting and improve generalization performance.\n",
        "* This includes controlling the learning rate, using shallow trees, and early stopping.\n",
        "\n",
        "By understanding these steps and their mathematical underpinnings, one can develop a clear intuition of how Gradient Boosting algorithm works and how it constructs the ensemble of weak learners to improve predictive performance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Dn50RO_ydt0f"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U0MDb0b5eXXi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}